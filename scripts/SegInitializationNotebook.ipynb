{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/home/lalith/.local/lib/python3.8/site-packages/tensorboard/compat/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Notebook whic\n",
    "# 1. Downloads weights\n",
    "# 2. Initializes model and imports weights\n",
    "# 3. Performs test time evaluation of videos (already preprocessed with ConvertDICOMToAVI.ipynb)\n",
    "\n",
    "import re\n",
    "import os, os.path\n",
    "from os.path import splitext\n",
    "import pydicom as dicom\n",
    "import numpy as np\n",
    "from pydicom.uid import UID, generate_uid\n",
    "import shutil\n",
    "from multiprocessing import dummy as multiprocessing\n",
    "import time\n",
    "import subprocess\n",
    "import datetime\n",
    "from datetime import date\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from shutil import copy\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import echonet\n",
    "\n",
    "import wget \n",
    "\n",
    "#destinationFolder = \"/Users/davidouyang/Dropbox/Echo Research/CodeBase/Output\"\n",
    "destinationFolder = \"/home/lalith/echonet/dynamic/Output_test\"\n",
    "#videosFolder = \"/Users/davidouyang/Dropbox/Echo Research/CodeBase/a4c-video-dir\"\n",
    "videosFolder = \"/mnt/storage/Anonymized_echo_AP4\"\n",
    "#DestinationForWeights = \"/Users/davidouyang/Dropbox/Echo Research/CodeBase/EchoNetDynamic-Weights\"\n",
    "DestinationForWeights = \"/home/lalith/echonet/dynamic/output/segmentation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder at  /home/lalith/echonet/dynamic/EchoNetDynamic-Weights  to store weights\n",
      "Downloading Segmentation Weights,  https://github.com/douyang/EchoNetDynamic/releases/download/v1.0.0/deeplabv3_resnet50_random.pt  to  /home/lalith/echonet/dynamic/EchoNetDynamic-Weights/deeplabv3_resnet50_random.pt\n",
      "Downloading EF Weights,  https://github.com/douyang/EchoNetDynamic/releases/download/v1.0.0/r2plus1d_18_32_2_pretrained.pt  to  /home/lalith/echonet/dynamic/EchoNetDynamic-Weights/r2plus1d_18_32_2_pretrained.pt\n"
     ]
    }
   ],
   "source": [
    "# Download model weights\n",
    "\n",
    "if os.path.exists(DestinationForWeights):\n",
    "    print(\"The weights are at\", DestinationForWeights)\n",
    "else:\n",
    "    print(\"Creating folder at \", DestinationForWeights, \" to store weights\")\n",
    "    os.mkdir(DestinationForWeights)\n",
    "    \n",
    "segmentationWeightsURL = 'https://github.com/douyang/EchoNetDynamic/releases/download/v1.0.0/deeplabv3_resnet50_random.pt'\n",
    "ejectionFractionWeightsURL = 'https://github.com/douyang/EchoNetDynamic/releases/download/v1.0.0/r2plus1d_18_32_2_pretrained.pt'\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.join(DestinationForWeights, os.path.basename(segmentationWeightsURL))):\n",
    "    print(\"Downloading Segmentation Weights, \", segmentationWeightsURL,\" to \",os.path.join(DestinationForWeights,os.path.basename(segmentationWeightsURL)))\n",
    "    filename = wget.download(segmentationWeightsURL, out = DestinationForWeights)\n",
    "else:\n",
    "    print(\"Segmentation Weights already present\")\n",
    "    \n",
    "if not os.path.exists(os.path.join(DestinationForWeights, os.path.basename(ejectionFractionWeightsURL))):\n",
    "    print(\"Downloading EF Weights, \", ejectionFractionWeightsURL,\" to \",os.path.join(DestinationForWeights,os.path.basename(ejectionFractionWeightsURL)))\n",
    "    filename = wget.download(ejectionFractionWeightsURL, out = DestinationForWeights)\n",
    "else:\n",
    "    print(\"EF Weights already present\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 235, in _feed\n",
      "    close()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 266, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from  /home/lalith/echonet/dynamic/EchoNetDynamic-Weights/r2plus1d_18_32_2_pretrained\n",
      "cuda is available, original weights\n",
      "EXTERNAL_TEST ['0X11109B2A42A4C76E.avi']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "run_epoch() got an unexpected keyword argument 'blocks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f950ce556f99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mechonet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: run_epoch() got an unexpected keyword argument 'blocks'"
     ]
    }
   ],
   "source": [
    "# Initialize and Run EF model\n",
    "\n",
    "frames = 32\n",
    "period = 1 #2\n",
    "batch_size = 20\n",
    "model = torchvision.models.video.r2plus1d_18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 1)\n",
    "\n",
    "\n",
    "\n",
    "print(\"loading weights from \", os.path.join(DestinationForWeights, \"r2plus1d_18_32_2_pretrained\"))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available, original weights\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    checkpoint = torch.load(os.path.join(DestinationForWeights, os.path.basename(ejectionFractionWeightsURL)))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "else:\n",
    "    print(\"cuda is not available, cpu weights\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    checkpoint = torch.load(os.path.join(DestinationForWeights, os.path.basename(ejectionFractionWeightsURL)), map_location = \"cpu\")\n",
    "    state_dict_cpu = {k[7:]: v for (k, v) in checkpoint['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict_cpu)\n",
    "\n",
    "\n",
    "# try some random weights: final_r2+1d_model_regression_EF_sgd_skip1_32frames.pth.tar\n",
    "# scp ouyangd@arthur2:~/Echo-Tracing-Analysis/final_r2+1d_model_regression_EF_sgd_skip1_32frames.pth.tar \"C:\\Users\\Windows\\Dropbox\\Echo Research\\CodeBase\\EchoNetDynamic-Weights\"\n",
    "#Weights = \"final_r2+1d_model_regression_EF_sgd_skip1_32frames.pth.tar\"\n",
    "\n",
    "\n",
    "output = os.path.join(destinationFolder, \"cedars_ef_output.csv\")\n",
    "\n",
    "ds = echonet.datasets.Echo(split = \"external_test\", external_test_location = videosFolder)\n",
    "print(ds.split, ds.fnames)\n",
    "\n",
    "mean, std = echonet.utils.get_mean_and_std(ds)\n",
    "\n",
    "kwargs = {\"target_type\": \"EF\",\n",
    "          \"mean\": mean,\n",
    "          \"std\": std,\n",
    "          \"length\": frames,\n",
    "          \"period\": period,\n",
    "          }\n",
    "\n",
    "ds = echonet.datasets.Echo(split = \"external_test\", external_test_location = videosFolder, **kwargs)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(ds, batch_size = 1, num_workers = 5, shuffle = True, pin_memory=(device.type == \"cuda\"))\n",
    "loss, yhat, y = echonet.utils.video.run_epoch(model, test_dataloader, \"test\", None, device, save_all=True, blocks=25)\n",
    "\n",
    "with open(output, \"w\") as g:\n",
    "    for (filename, pred) in zip(ds.fnames, yhat):\n",
    "        for (i,p) in enumerate(pred):\n",
    "            g.write(\"{},{},{:.4f}\\n\".format(filename, i, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lalith/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/lalith/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from  /home/lalith/echonet/dynamic/EchoNetDynamic-Weights/deeplabv3_resnet50_random\n",
      "cuda is available, original weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]<ipython-input-9-e1a9b4307ba9>:115: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all axes decorations. \n",
      "  plt.tight_layout()\n",
      "<ipython-input-9-e1a9b4307ba9>:115: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all axes decorations. \n",
      "  plt.tight_layout()\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 112 3\n",
      "112 112 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and Run Segmentation model\n",
    "import skimage.draw\n",
    "import tqdm\n",
    "import sklearn\n",
    "import scipy\n",
    "torch.cuda.empty_cache()\n",
    "batch_size = 20\n",
    "\n",
    "videosFolder = \"/home/lalith/scratch/converted_camus\"\n",
    "\n",
    "model_name = 'deeplabv3_resnet50'\n",
    "model = torchvision.models.segmentation.__dict__[model_name](pretrained=False, aux_loss=False)\n",
    "model.classifier[-1] = torch.nn.Conv2d(model.classifier[-1].in_channels, 1, kernel_size=model.classifier[-1].kernel_size)  # change number of outputs to 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"loading weights from \", os.path.join(DestinationForWeights, \"deeplabv3_resnet50_random\"))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available, original weights\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    checkpoint = torch.load(os.path.join(DestinationForWeights, os.path.basename(segmentationWeightsURL)))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "else:\n",
    "    print(\"cuda is not available, cpu weights\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    checkpoint = torch.load(os.path.join(DestinationForWeights, os.path.basename(segmentationWeightsURL)), map_location = \"cpu\")\n",
    "    state_dict_cpu = {k[7:]: v for (k, v) in checkpoint['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict_cpu)\n",
    "\n",
    "\n",
    "def collate_fn(x):\n",
    "    x, f = zip(*x)\n",
    "    i = list(map(lambda t: t.shape[1], x))\n",
    "    x = torch.as_tensor(np.swapaxes(np.concatenate(x, 1), 0, 1))\n",
    "    return x, f, i\n",
    "mean = np.array([31.834011, 31.95879,  32.082172])\n",
    "std = np.array([48.866325, 49.137333, 49.361984])\n",
    "dataloader = torch.utils.data.DataLoader(echonet.datasets.Echo(split=\"external_test\", external_test_location = videosFolder, target_type=[\"Filename\"], length=None, period=1, mean=mean, std=std),\n",
    "                                         batch_size=10, num_workers=0, shuffle=False, pin_memory=(device.type == \"cuda\"), collate_fn=collate_fn)\n",
    "\n",
    "output = destinationFolder\n",
    "model.eval()\n",
    "\n",
    "os.makedirs(os.path.join(output, \"videos\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output, \"size\"), exist_ok=True)\n",
    "echonet.utils.latexify()\n",
    "flip = False\n",
    "with torch.no_grad():\n",
    "    with open(os.path.join(output, \"size.csv\"), \"w\") as g:\n",
    "        g.write(\"Filename,Frame,Size,ComputerSmall\\n\")\n",
    "\n",
    "        for (x, (filenames), length) in tqdm.tqdm(dataloader):\n",
    "            # print(x, filenames, length)\n",
    "            if flip:\n",
    "                x = torch.flip(x,[3])\n",
    "            # Run segmentation model on blocks of frames one-by-one\n",
    "            # The whole concatenated video may be too long to run together\n",
    "            y = np.concatenate([model(x[i:(i + batch_size), :, :, :].to(device))[\"out\"].detach().cpu().numpy() for i in range(0, x.shape[0], batch_size)])\n",
    "\n",
    "            start = 0\n",
    "            \n",
    "            x = x.numpy()\n",
    "            for (i, (filename, offset)) in enumerate(zip(filenames, length)):\n",
    "                # Extract one video and segmentation predictions\n",
    "                video = x[start:(start + offset), ...]\n",
    "                logit = y[start:(start + offset), 0, :, :]\n",
    "\n",
    "                # Un-normalize video\n",
    "                video *= std.reshape(1, 3, 1, 1)\n",
    "                video += mean.reshape(1, 3, 1, 1)\n",
    "\n",
    "                # Get frames, channels, height, and width\n",
    "                f, c, h, w = video.shape  # pylint: disable=W0612\n",
    "                print(w,h,c)\n",
    "                assert c == 3\n",
    "\n",
    "                # Put two copies of the video side by side\n",
    "                video = np.concatenate((video, video), 3)\n",
    "\n",
    "                # If a pixel is in the segmentation, saturate blue channel\n",
    "                # Leave alone otherwise\n",
    "                video[:, 0, :, w:] = np.maximum(255. * (logit > 0), video[:, 0, :, w:])  # pylint: disable=E1111\n",
    "\n",
    "                # Add blank canvas under pair of videos\n",
    "                video = np.concatenate((video, np.zeros_like(video)), 2)\n",
    "\n",
    "                # Compute size of segmentation per frame\n",
    "                size = (logit > 0).sum((1, 2))\n",
    "\n",
    "                # Identify systole frames with peak detection\n",
    "                trim_min = sorted(size)[round(len(size) ** 0.05)]\n",
    "                trim_max = sorted(size)[round(len(size) ** 0.95)]\n",
    "                trim_range = trim_max - trim_min\n",
    "                systole = set(scipy.signal.find_peaks(-size, distance=20, prominence=(0.50 * trim_range))[0])\n",
    "\n",
    "                # Write sizes and frames to file\n",
    "                for (frame, s) in enumerate(size):\n",
    "                    # g.write(\"{},{},{},{}\\n\".format(filename, frame, s, 1 if x in  else 0))\n",
    "                    g.write(\"{},{},{},{}\\n\".format(filename, frame, s, 1 if frame in systole else 0))\n",
    "\n",
    "\n",
    "                # Plot sizes\n",
    "                fig = plt.figure(figsize=(size.shape[0] / 50 * 1.5, 3))\n",
    "                plt.scatter(np.arange(size.shape[0]) / 50, size, s=1)\n",
    "                ylim = plt.ylim()\n",
    "                for s in systole:\n",
    "                    plt.plot(np.array([s, s]) / 50, ylim, linewidth=1)\n",
    "                plt.ylim(ylim)\n",
    "                plt.title(os.path.splitext(filename)[0])\n",
    "                plt.xlabel(\"Seconds\")\n",
    "                plt.ylabel(\"Size (pixels)\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output, \"size\", os.path.splitext(filename)[0] + \".pdf\"))\n",
    "                plt.close(fig)\n",
    "\n",
    "                # Normalize size to [0, 1]\n",
    "                size -= size.min()\n",
    "                size = size / size.max()\n",
    "                size = 1 - size\n",
    "\n",
    "                # Iterate the frames in this video\n",
    "                for (f, s) in enumerate(size):\n",
    "\n",
    "                    # On all frames, mark a pixel for the size of the frame\n",
    "                    video[:, :, int(round(115 + 100 * s)), int(round(f / len(size) * 200 + 10))] = 255.\n",
    "\n",
    "                    if f in systole:\n",
    "                        # If frame is computer-selected systole, mark with a line\n",
    "                        video[:, :, 115:224, int(round(f / len(size) * 200 + 10))] = 255.\n",
    "\n",
    "                    def dash(start, stop, on=10, off=10):\n",
    "                        buf = []\n",
    "                        x = start\n",
    "                        while x < stop:\n",
    "                            buf.extend(range(x, x + on))\n",
    "                            x += on\n",
    "                            x += off\n",
    "                        buf = np.array(buf)\n",
    "                        buf = buf[buf < stop]\n",
    "                        return buf\n",
    "                    d = dash(115, 224)\n",
    "\n",
    "                    # if f == large_index[i]:\n",
    "                    #     # If frame is human-selected diastole, mark with green dashed line on all frames\n",
    "                    #     video[:, :, d, int(round(f / len(size) * 200 + 10))] = np.array([0, 225, 0]).reshape((1, 3, 1))\n",
    "                    # if f == small_index[i]:\n",
    "                    #     # If frame is human-selected systole, mark with red dashed line on all frames\n",
    "                    #     video[:, :, d, int(round(f / len(size) * 200 + 10))] = np.array([0, 0, 225]).reshape((1, 3, 1))\n",
    "\n",
    "                    # Get pixels for a circle centered on the pixel\n",
    "                    r, c = skimage.draw.disk((int(round(115 + 100 * s)), int(round(f / len(size) * 200 + 10))), 4.1)\n",
    "\n",
    "                    # On the frame that's being shown, put a circle over the pixel\n",
    "                    video[f, :, r, c] = 255.\n",
    "\n",
    "                # Rearrange dimensions and save\n",
    "                video = video.transpose(1, 0, 2, 3)\n",
    "                video = video.astype(np.uint8)\n",
    "                echonet.utils.savevideo(os.path.join(output, \"videos\", 'result_'+filename), video, 50)\n",
    "\n",
    "                # Move to next video\n",
    "                start += offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for just getting output\n",
    "import pathlib\n",
    "import tqdm\n",
    "import scipy\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "model_name = 'deeplabv3_resnet50'\n",
    "model = torchvision.models.segmentation.__dict__[model_name](pretrained=False, aux_loss=False)\n",
    "model.classifier[-1] = torch.nn.Conv2d(model.classifier[-1].in_channels, 1, kernel_size=model.classifier[-1].kernel_size)  # change number of outputs to 1\n",
    "def smooth_segmentation(x,alpha):\n",
    "    \"\"\"\n",
    "    This function smooths the segmentation over between frames.\n",
    "    The logic is:\n",
    "    the nth frame = alpha * the nth frame + (1-alpha) * the n-1th frame\n",
    "    \"\"\"\n",
    "    final = [x[0]]\n",
    "    for i in x[1:]:\n",
    "        final.append(alpha*i+(1-alpha)*final[-1])\n",
    "    return np.array(final)\n",
    "\n",
    "\n",
    "print(\"loading weights from \", os.path.join(DestinationForWeights, \"deeplabv3_resnet50_random\"))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available, original weights\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    checkpoint = torch.load(os.path.join(DestinationForWeights, os.path.basename(segmentationWeightsURL)))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "else:\n",
    "    print(\"cuda is not available, cpu weights\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    checkpoint = torch.load(os.path.join(DestinationForWeights, os.path.basename(segmentationWeightsURL)), map_location = \"cpu\")\n",
    "    state_dict_cpu = {k[7:]: v for (k, v) in checkpoint['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict_cpu)\n",
    "\n",
    "videosFolder = \"/home/lalith/scratch/converted_echo_abnormal_AP4\"\n",
    "\n",
    "ds = echonet.datasets.Echo(split = \"external_test\", external_test_location = videosFolder)\n",
    "print(ds.split, ds.fnames)\n",
    "\n",
    "mean, std = echonet.utils.get_mean_and_std(ds,batch_size=1)\n",
    "\n",
    "def collate_fn(x):\n",
    "    x, f = zip(*x)\n",
    "    i = list(map(lambda t: t.shape[1], x))\n",
    "    x = torch.as_tensor(np.swapaxes(np.concatenate(x, 1), 0, 1))\n",
    "    return x, f, i\n",
    "\n",
    "test_ds = echonet.datasets.Echo(split=\"external_test\", external_test_location = videosFolder, target_type=[\"Filename\"], length=None, period=1, mean=mean, std=std)\n",
    "dataloader = torch.utils.data.DataLoader(test_ds, batch_size=1, num_workers=8, shuffle=False, pin_memory=(device.type == \"cuda\"), collate_fn=collate_fn)\n",
    "block = 1024\n",
    "model.eval()\n",
    "alpha = 0.9\n",
    "flip = True\n",
    "with torch.no_grad():\n",
    "    for (x, f, i) in tqdm.tqdm(dataloader):\n",
    "        if flip:\n",
    "            x = torch.flip(x,[3])\n",
    "        x = x.to(device)\n",
    "        y = np.concatenate([model(x[i:(i + block), :, :, :])[\"out\"].detach().cpu().numpy() for i in range(0, x.shape[0], block)]).astype(np.float16)\n",
    "        start = 0\n",
    "        oldvideo = x.cpu().numpy().copy()\n",
    "        oldvideo = oldvideo * std.reshape(1, 3, 1, 1)\n",
    "        oldvideo = oldvideo + mean.reshape(1, 3, 1, 1)\n",
    "        newvideo = oldvideo.copy()\n",
    "        output_folder = os.path.join(destinationFolder, 'videos')\n",
    "        # This is the line that smooths the segmentation\n",
    "        newvideo[:,2,:,:] = np.maximum(newvideo[:,2,:,:], 255. * (smooth_segmentation(y[:, 0, :, :],alpha) > 0))\n",
    "        for (filename, offset) in zip(f,i):\n",
    "            # This line also smooths the segmentation\n",
    "            np.save(os.path.join(output_folder, os.path.splitext(filename)[0]), smooth_segmentation(y[start:(start+offset), 0, :, :],alpha))\n",
    "            #plain videos\n",
    "            echonet.utils.savevideo(os.path.join(output_folder,os.path.splitext(filename)[0] + \".avi\"), np.transpose(newvideo[start:(start+offset), :, :, :],(1,0,2,3)).astype(np.uint8), 65)\n",
    "            # shutil.rmtree('temp')\n",
    "            print(os.path.join(output_folder,os.path.splitext(filename)[0] + \".avi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
